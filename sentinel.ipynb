{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f92eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85291163",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "SENTINEL - NeuralProphet Baseline Implementation\n",
    "===============================================\n",
    "Plataforma inteligente para monitoreo industrial y detecci√≥n predictiva de fallas\n",
    "\n",
    "Autor: Equipo SENTINEL\n",
    "Fecha: Mayo 2025\n",
    "Objetivo: Establecer baseline de forecasting con NeuralProphet para series temporales industriales\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# 1. PREPARACI√ìN DEL ENTORNO Y DEPENDENCIAS\n",
    "# =============================================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Instalaci√≥n de dependencias (ejecutar solo si es necesario)\n",
    "\"\"\"\n",
    "%pip install neuralprophet pandas numpy matplotlib plotly seaborn scikit-learn nbformat\n",
    "\"\"\"\n",
    "\n",
    "# Import de librer√≠as principales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# NeuralProphet y m√©tricas\n",
    "from neuralprophet import NeuralProphet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
    "print(f\"üìä Versi√≥n de pandas: {pd.__version__}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. GENERACI√ìN DE DATOS SIMULADOS REPRESENTATIVOS\n",
    "# =============================================================================\n",
    "\n",
    "def generate_industrial_sensor_data(start_date='2023-01-01', periods=8760, freq='H'):\n",
    "    \"\"\"\n",
    "    Genera datos simulados de sensores industriales para SENTINEL\n",
    "    \n",
    "    Par√°metros:\n",
    "    - start_date: fecha de inicio\n",
    "    - periods: n√∫mero de per√≠odos (8760 = 1 a√±o en horas)\n",
    "    - freq: frecuencia temporal ('H' = horaria)\n",
    "    \n",
    "    Retorna:\n",
    "    - DataFrame con datos de sensores simulados\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crear √≠ndice temporal\n",
    "    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n",
    "    \n",
    "    # Par√°metros base para simulaci√≥n realista\n",
    "    np.random.seed(42)  # Para reproducibilidad, usar una semilla fija, una semilla significa # que los n√∫meros aleatorios generados ser√°n los mismos en cada ejecuci√≥n\n",
    "    \n",
    "    # === PRESI√ìN (variable objetivo principal) ===\n",
    "    # Tendencia base con estacionalidad diaria y semanal\n",
    "    trend = np.linspace(100, 110, periods)  # Tendencia ascendente leve\n",
    "    daily_season = 5 * np.sin(2 * np.pi * np.arange(periods) / 24)  # Ciclo diario\n",
    "    weekly_season = 3 * np.sin(2 * np.pi * np.arange(periods) / (24*7))  # Ciclo semanal\n",
    "    \n",
    "    # Ruido realista con algunos picos an√≥malos\n",
    "    noise = np.random.normal(0, 2, periods)\n",
    "    \n",
    "    # Simular eventos de mantenimiento (ca√≠das programadas)\n",
    "    maintenance_events = np.random.choice(periods, size=12, replace=False)\n",
    "    for event in maintenance_events:\n",
    "        if event + 8 < periods:  # 8 horas de mantenimiento\n",
    "            trend[event:event+8] -= 15\n",
    "    \n",
    "    # Presi√≥n total\n",
    "    pressure = trend + daily_season + weekly_season + noise\n",
    "    pressure = np.clip(pressure, 85, 125)  # L√≠mites realistas\n",
    "    \n",
    "    # === VARIABLES EX√ìGENAS ===\n",
    "    # Temperatura ambiente (afecta la presi√≥n)\n",
    "    temp_base = 25 + 10 * np.sin(2 * np.pi * np.arange(periods) / (24*365))  # Estacional anual\n",
    "    temp_daily = 8 * np.sin(2 * np.pi * np.arange(periods) / 24)  # Variaci√≥n diaria\n",
    "    temperature = temp_base + temp_daily + np.random.normal(0, 2, periods)\n",
    "    \n",
    "    # Flujo (correlacionado con presi√≥n)\n",
    "    flow_base = 50 + 0.3 * (pressure - 100)  # Correlaci√≥n con presi√≥n\n",
    "    flow = flow_base + np.random.normal(0, 3, periods)\n",
    "    flow = np.clip(flow, 30, 80)\n",
    "    \n",
    "    # Vibraci√≥n (indicador de desgaste)\n",
    "    vibration_trend = np.linspace(2, 4, periods)  # Incremento gradual por desgaste\n",
    "    vibration = vibration_trend + np.random.exponential(0.5, periods)\n",
    "    \n",
    "    # Humedad\n",
    "    humidity = 60 + 20 * np.sin(2 * np.pi * np.arange(periods) / (24*365)) + np.random.normal(0, 5, periods)\n",
    "    humidity = np.clip(humidity, 30, 90)\n",
    "    \n",
    "    # === CREAR DATAFRAME ===\n",
    "    df = pd.DataFrame({\n",
    "        'ds': dates,  # Timestamp (formato requerido por NeuralProphet)\n",
    "        'y': pressure,  # Variable objetivo\n",
    "        # Variables ex√≥genas\n",
    "        'temperature': temperature,\n",
    "        'flow': flow,\n",
    "        'vibration': vibration,\n",
    "        'humidity': humidity,\n",
    "        # Variables categ√≥ricas\n",
    "        'hour': dates.hour,\n",
    "        'day_of_week': dates.dayofweek,\n",
    "        'month': dates.month,\n",
    "        'is_weekend': (dates.dayofweek >= 5).astype(int),\n",
    "        'is_maintenance_day': 0  # Se llenar√° seg√∫n eventos\n",
    "    })\n",
    "    \n",
    "    # Marcar d√≠as de mantenimiento\n",
    "    for event in maintenance_events:\n",
    "        df.loc[event:event+8, 'is_maintenance_day'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generar datos simulados\n",
    "print(\"üîÑ Generando datos simulados de sensores industriales...\")\n",
    "df_raw = generate_industrial_sensor_data(start_date='2023-01-01', periods=8760, freq='H')\n",
    "\n",
    "print(f\"‚úÖ Datos generados: {len(df_raw)} registros\")\n",
    "print(f\"üìÖ Per√≠odo: {df_raw['ds'].min()} a {df_raw['ds'].max()}\")\n",
    "print(\"\\nüìä Primeras 5 filas:\")\n",
    "print(df_raw.head())\n",
    "\n",
    "print(\"\\nüìà Estad√≠sticas descriptivas:\")\n",
    "print(df_raw.describe())\n",
    "\n",
    "# =============================================================================\n",
    "# 3. AN√ÅLISIS EXPLORATORIO DE DATOS (EDA)\n",
    "# =============================================================================\n",
    "\n",
    "def plot_eda_analysis(df):\n",
    "    \"\"\"Generar gr√°ficos de an√°lisis exploratorio\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=[\n",
    "            'Serie Temporal - Presi√≥n (Variable Objetivo)',\n",
    "            'Distribuci√≥n de Presi√≥n',\n",
    "            'Correlaci√≥n: Presi√≥n vs Temperatura',\n",
    "            'Estacionalidad Diaria',\n",
    "            'Variables Ex√≥genas vs Tiempo',\n",
    "            'Matriz de Correlaci√≥n'\n",
    "        ],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": True}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Serie temporal principal\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['ds'], y=df['y'], name='Presi√≥n', line=dict(color='blue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Histograma de distribuci√≥n\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df['y'], name='Distribuci√≥n', nbinsx=50, marker_color='lightblue'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Scatter plot presi√≥n vs temperatura\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['temperature'], y=df['y'], mode='markers', \n",
    "                  name='Presi√≥n vs Temp', marker=dict(color='red', size=3, opacity=0.6)),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Patr√≥n diario (promedio por hora)\n",
    "    hourly_avg = df.groupby('hour')['y'].mean()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=hourly_avg.index, y=hourly_avg.values, \n",
    "                  name='Patr√≥n Diario', line=dict(color='green')),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # 5. Variables ex√≥genas\n",
    "    sample_data = df.iloc[::100]  # Muestrear para visualizaci√≥n\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=sample_data['ds'], y=sample_data['temperature'], \n",
    "                  name='Temperatura', line=dict(color='orange')),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=sample_data['ds'], y=sample_data['flow'], \n",
    "                  name='Flujo', line=dict(color='purple'), yaxis='y2'),\n",
    "        row=3, col=1, secondary_y=True\n",
    "    )\n",
    "    \n",
    "    # Actualizar layouts\n",
    "    fig.update_layout(height=1200, title_text=\"üìä SENTINEL - An√°lisis Exploratorio de Datos\")\n",
    "    fig.update_xaxes(title_text=\"Fecha\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Presi√≥n (PSI)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Frecuencia\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Presi√≥n\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Presi√≥n Promedio\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Temperatura (¬∞C)\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Flujo (L/min)\", secondary_y=True, row=3, col=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Generar an√°lisis exploratorio\n",
    "print(\"üîç Realizando an√°lisis exploratorio de datos...\")\n",
    "eda_fig = plot_eda_analysis(df_raw)\n",
    "eda_fig.show()\n",
    "\n",
    "# Matriz de correlaci√≥n\n",
    "corr_cols = ['y', 'temperature', 'flow', 'vibration', 'humidity']\n",
    "correlation_matrix = df_raw[corr_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('üîó Matriz de Correlaci√≥n - Variables SENTINEL')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìã Correlaciones con la variable objetivo (Presi√≥n):\")\n",
    "target_corr = correlation_matrix['y'].drop('y').sort_values(key=abs, ascending=False)\n",
    "for var, corr in target_corr.items():\n",
    "    print(f\"  ‚Ä¢ {var}: {corr:.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. PREPARACI√ìN DE DATOS PARA NEURALPROPHET\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_data_for_neuralprophet(df, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepara los datos para el entrenamiento con NeuralProphet\n",
    "    \n",
    "    Par√°metros:\n",
    "    - df: DataFrame con los datos\n",
    "    - test_size: proporci√≥n de datos para testing\n",
    "    \n",
    "    Retorna:\n",
    "    - df_train, df_test: DataFrames de entrenamiento y prueba\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ordenar por fecha\n",
    "    df = df.sort_values('ds').reset_index(drop=True)\n",
    "    \n",
    "    # Calcular punto de divisi√≥n\n",
    "    split_point = int(len(df) * (1 - test_size))\n",
    "    \n",
    "    # Dividir datos\n",
    "    df_train = df.iloc[:split_point].copy()\n",
    "    df_test = df.iloc[split_point:].copy()\n",
    "    \n",
    "    print(f\"üìä Divisi√≥n de datos:\")\n",
    "    print(f\"  ‚Ä¢ Entrenamiento: {len(df_train)} registros ({df_train['ds'].min()} a {df_train['ds'].max()})\")\n",
    "    print(f\"  ‚Ä¢ Prueba: {len(df_test)} registros ({df_test['ds'].min()} a {df_test['ds'].max()})\")\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "# Preparar datos\n",
    "df_train, df_test = prepare_data_for_neuralprophet(df_raw, test_size=0.2)\n",
    "\n",
    "# Seleccionar variables para el modelo\n",
    "# Variables ex√≥genas m√°s relevantes basadas en correlaci√≥n\n",
    "key_regressors = ['temperature', 'flow', 'vibration']\n",
    "\n",
    "print(f\"\\nüéØ Variables seleccionadas para el modelo:\")\n",
    "print(f\"  ‚Ä¢ Variable objetivo: y (presi√≥n)\")\n",
    "print(f\"  ‚Ä¢ Variables ex√≥genas: {key_regressors}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. CONFIGURACI√ìN Y ENTRENAMIENTO DE NEURALPROPHET\n",
    "# =============================================================================\n",
    "\n",
    "def create_and_train_neuralprophet_model(df_train, regressors=None):\n",
    "    \"\"\"\n",
    "    Crea y entrena un modelo NeuralProphet optimizado para datos industriales\n",
    "    \n",
    "    Par√°metros:\n",
    "    - df_train: DataFrame de entrenamiento\n",
    "    - regressors: lista de variables ex√≥genas\n",
    "    \n",
    "    Retorna:\n",
    "    - modelo entrenado\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üß† Configurando modelo NeuralProphet...\")\n",
    "    \n",
    "    # Configuraci√≥n del modelo optimizada para datos industriales\n",
    "    model = NeuralProphet(\n",
    "        # Configuraci√≥n de crecimiento\n",
    "        growth='linear',  # Crecimiento lineal para datos industriales\n",
    "        \n",
    "        # Estacionalidades\n",
    "        yearly_seasonality=True,   # Estacionalidad anual\n",
    "        weekly_seasonality=True,   # Estacionalidad semanal\n",
    "        daily_seasonality=True,    # Estacionalidad diaria (cr√≠tica para procesos industriales)\n",
    "        \n",
    "        # Configuraci√≥n de lags autorregresivos\n",
    "        n_lags=48,  # 48 horas de historia (2 d√≠as)\n",
    "        \n",
    "        # Configuraci√≥n de la red neuronal\n",
    "        num_hidden_layers=2,\n",
    "        d_hidden=64,\n",
    "        \n",
    "        # Configuraci√≥n de entrenamiento\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        learning_rate=0.01,\n",
    "        \n",
    "        # Normalizaci√≥n\n",
    "        normalize='standardize',\n",
    "        \n",
    "        # Configuraci√≥n de validaci√≥n\n",
    "        newer_samples_weight=1.2,  # Dar m√°s peso a muestras recientes\n",
    "        \n",
    "        # Reducir verbosidad\n",
    "        log_level='ERROR'\n",
    "    )\n",
    "    \n",
    "    # Agregar variables ex√≥genas si se proporcionan\n",
    "    if regressors:\n",
    "        for regressor in regressors:\n",
    "            print(f\"  ‚ûï Agregando variable ex√≥gena: {regressor}\")\n",
    "            model.add_lagged_regressor(regressor, n_lags=24)  # 24 horas de lags\n",
    "    \n",
    "    print(\"üîÑ Iniciando entrenamiento del modelo...\")\n",
    "    \n",
    "    # Preparar datos de entrenamiento\n",
    "    df_model_train = df_train[['ds', 'y'] + (regressors or [])].copy()\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    metrics = model.fit(df_model_train, freq='H', validation_df=None)\n",
    "    \n",
    "    print(\"‚úÖ Entrenamiento completado!\")\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "# Entrenar modelo\n",
    "model, training_metrics = create_and_train_neuralprophet_model(\n",
    "    df_train, \n",
    "    regressors=key_regressors\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. GENERACI√ìN DE PREDICCIONES Y EVALUACI√ìN\n",
    "# =============================================================================\n",
    "\n",
    "def generate_predictions_and_evaluate(model, df_train, df_test, regressors=None):\n",
    "    \"\"\"\n",
    "    Genera predicciones y eval√∫a el desempe√±o del modelo\n",
    "    \n",
    "    Par√°metros:\n",
    "    - model: modelo entrenado\n",
    "    - df_train: datos de entrenamiento\n",
    "    - df_test: datos de prueba\n",
    "    - regressors: variables ex√≥genas\n",
    "    \n",
    "    Retorna:\n",
    "    - predicciones, m√©tricas de evaluaci√≥n\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîÆ Generando predicciones...\")\n",
    "    \n",
    "    # Preparar datos completos para predicci√≥n\n",
    "    df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
    "    df_model = df_full[['ds', 'y'] + (regressors or [])].copy()\n",
    "    \n",
    "    # Generar predicciones para todo el conjunto\n",
    "    forecast = model.predict(df_model)\n",
    "    \n",
    "    # Separar predicciones de entrenamiento y prueba\n",
    "    train_size = len(df_train)\n",
    "    forecast_train = forecast.iloc[:train_size]\n",
    "    forecast_test = forecast.iloc[train_size:]\n",
    "    \n",
    "    # Calcular m√©tricas de evaluaci√≥n\n",
    "    def calculate_metrics(y_true, y_pred, dataset_name):\n",
    "        \"\"\"Calcula m√©tricas de evaluaci√≥n\"\"\"\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        metrics = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape,\n",
    "            'R¬≤': r2\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìä M√©tricas - {dataset_name}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  ‚Ä¢ {metric}: {value:.4f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    # Evaluar en conjunto de entrenamiento\n",
    "    train_metrics = calculate_metrics(\n",
    "        df_train['y'].values, \n",
    "        forecast_train['yhat1'].values, \n",
    "        \"Entrenamiento\"\n",
    "    )\n",
    "    \n",
    "    # Evaluar en conjunto de prueba\n",
    "    test_metrics = calculate_metrics(\n",
    "        df_test['y'].values, \n",
    "        forecast_test['yhat1'].values, \n",
    "        \"Prueba\"\n",
    "    )\n",
    "    \n",
    "    return forecast, forecast_train, forecast_test, train_metrics, test_metrics\n",
    "\n",
    "# Generar predicciones y evaluar\n",
    "forecast, forecast_train, forecast_test, train_metrics, test_metrics = generate_predictions_and_evaluate(\n",
    "    model, df_train, df_test, key_regressors\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 7. VISUALIZACI√ìN DE RESULTADOS\n",
    "# =============================================================================\n",
    "\n",
    "def plot_comprehensive_results(df_train, df_test, forecast_train, forecast_test, model):\n",
    "    \"\"\"\n",
    "    Genera visualizaciones completas de los resultados del modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    # === GR√ÅFICO PRINCIPAL: PREDICCIONES VS REALIDAD ===\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=[\n",
    "            'üéØ Predicciones vs Realidad - Serie Completa',\n",
    "            'üîç Zoom: √öltimos 30 d√≠as de Prueba',\n",
    "            'üìä Residuales - Conjunto de Entrenamiento',\n",
    "            'üìä Residuales - Conjunto de Prueba',\n",
    "            'üìà Componentes del Modelo',\n",
    "            '‚ö° Distribuci√≥n de Errores'\n",
    "        ],\n",
    "        specs=[[{\"colspan\": 2}, None],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Serie temporal completa\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_train['ds'], y=df_train['y'], \n",
    "                  name='Datos Reales (Train)', line=dict(color='blue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_test['ds'], y=df_test['y'], \n",
    "                  name='Datos Reales (Test)', line=dict(color='darkblue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=forecast_train['ds'], y=forecast_train['yhat1'], \n",
    "                  name='Predicciones (Train)', line=dict(color='red', width=1)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=forecast_test['ds'], y=forecast_test['yhat1'], \n",
    "                  name='Predicciones (Test)', line=dict(color='orange', width=2)),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Intervalos de confianza\n",
    "    if 'yhat1_lower' in forecast_test.columns and 'yhat1_upper' in forecast_test.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=forecast_test['ds'], y=forecast_test['yhat1_upper'], \n",
    "                      fill=None, mode='lines', line_color='rgba(0,0,0,0)', \n",
    "                      showlegend=False),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=forecast_test['ds'], y=forecast_test['yhat1_lower'], \n",
    "                      fill='tonexty', mode='lines', line_color='rgba(0,0,0,0)', \n",
    "                      name='Intervalo Confianza', fillcolor='rgba(255,165,0,0.2)'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 2. Zoom a √∫ltimos 30 d√≠as de prueba\n",
    "    last_30_days = df_test.tail(720)  # 30 d√≠as * 24 horas\n",
    "    forecast_30_days = forecast_test.tail(720)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=last_30_days['ds'], y=last_30_days['y'], \n",
    "                  name='Real (30d)', line=dict(color='blue', width=3)),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=forecast_30_days['ds'], y=forecast_30_days['yhat1'], \n",
    "                  name='Predicci√≥n (30d)', line=dict(color='red', width=2)),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 3. Residuales de entrenamiento\n",
    "    train_residuals = df_train['y'].values - forecast_train['yhat1'].values\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_train['ds'], y=train_residuals, \n",
    "                  mode='markers', name='Residuales Train', \n",
    "                  marker=dict(color='green', size=2, opacity=0.6)),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", row=2, col=2)\n",
    "    \n",
    "    # 4. Residuales de prueba\n",
    "    test_residuals = df_test['y'].values - forecast_test['yhat1'].values\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_test['ds'], y=test_residuals, \n",
    "                  mode='markers', name='Residuales Test', \n",
    "                  marker=dict(color='red', size=3, opacity=0.7)),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", row=3, col=1)\n",
    "    \n",
    "    # 5. Distribuci√≥n de errores\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=test_residuals, name='Dist. Errores', \n",
    "                    nbinsx=30, marker_color='lightcoral'),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    # Actualizar layout\n",
    "    fig.update_layout(\n",
    "        height=1200,\n",
    "        title_text=\"üöÄ SENTINEL - Resultados del Modelo NeuralProphet\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Etiquetas de ejes\n",
    "    fig.update_yaxes(title_text=\"Presi√≥n (PSI)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Presi√≥n (PSI)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Residuales\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Residuales\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Frecuencia\", row=3, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Generar visualizaci√≥n completa\n",
    "results_fig = plot_comprehensive_results(df_train, df_test, forecast_train, forecast_test, model)\n",
    "results_fig.show()\n",
    "\n",
    "# === COMPONENTES DEL MODELO ===\n",
    "print(\"üìà Analizando componentes del modelo...\")\n",
    "\n",
    "# Generar componentes\n",
    "components = model.predict_components(df_train[['ds', 'y'] + key_regressors])\n",
    "\n",
    "# Plotear componentes\n",
    "fig_components = model.plot_components(\n",
    "    forecast.iloc[:len(df_train)],  # Solo datos de entrenamiento para componentes\n",
    "    components=components\n",
    ")\n",
    "fig_components.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 8. AN√ÅLISIS DE IMPORTANCIA DE VARIABLES\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_feature_importance(model, regressors):\n",
    "    \"\"\"\n",
    "    Analiza la importancia de las variables en el modelo\n",
    "    \"\"\"\n",
    "    print(\"üîç Analizando importancia de variables...\")\n",
    "    \n",
    "    # Obtener par√°metros del modelo\n",
    "    params = model.model.named_parameters()\n",
    "    \n",
    "    print(\"\\nüìä Variables incluidas en el modelo:\")\n",
    "    print(f\"  ‚Ä¢ Variable objetivo: y (presi√≥n)\")\n",
    "    print(f\"  ‚Ä¢ Lags autorregresivos: {model.config_lagged_regressors.get('y', {}).get('n_lags', 0)}\")\n",
    "    \n",
    "    for regressor in regressors:\n",
    "        n_lags = model.config_lagged_regressors.get(regressor, {}).get('n_lags', 0)\n",
    "        print(f\"  ‚Ä¢ {regressor}: {n_lags} lags\")\n",
    "    \n",
    "    return params\n",
    "\n",
    "# Analizar importancia\n",
    "importance_analysis = analyze_feature_importance(model, key_regressors)\n",
    "\n",
    "# =============================================================================\n",
    "# 9. PREDICCIONES FUTURAS\n",
    "# =============================================================================\n",
    "\n",
    "def generate_future_predictions(model, df_full, periods=168, regressors=None):\n",
    "    \"\"\"\n",
    "    Genera predicciones futuras (168 horas = 1 semana)\n",
    "    \n",
    "    Par√°metros:\n",
    "    - model: modelo entrenado\n",
    "    - df_full: datos completos\n",
    "    - periods: per√≠odos a predecir\n",
    "    - regressors: variables ex√≥genas\n",
    "    \n",
    "    Retorna:\n",
    "    - DataFrame con predicciones futuras\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîÆ Generando predicciones futuras para {periods} horas ({periods//24} d√≠as)...\")\n",
    "    \n",
    "    # Crear fechas futuras\n",
    "    last_date = df_full['ds'].max()\n",
    "    future_dates = pd.date_range(\n",
    "        start=last_date + timedelta(hours=1),\n",
    "        periods=periods,\n",
    "        freq='H'\n",
    "    )\n",
    "    \n",
    "    # Crear DataFrame futuro\n",
    "    df_future = pd.DataFrame({'ds': future_dates})\n",
    "    \n",
    "    # Si hay variables ex√≥genas, necesitamos proyectarlas\n",
    "    if regressors:\n",
    "        print(\"‚ö†Ô∏è  Proyectando variables ex√≥genas usando tendencias hist√≥ricas...\")\n",
    "        \n",
    "        for regressor in regressors:\n",
    "            # Usar los √∫ltimos valores como base y agregar ruido controlado\n",
    "            last_values = df_full[regressor].tail(24).values  # √öltimas 24 horas\n",
    "            mean_val = np.mean(last_values)\n",
    "            std_val = np.std(last_values)\n",
    "            \n",
    "            # Generar valores futuros con patr√≥n similar\n",
    "            if regressor == 'temperature':\n",
    "                # Temperatura con patr√≥n diario\n",
    "                future_vals = mean_val + 3 * np.sin(2 * np.pi * np.arange(periods) / 24)\n",
    "                future_vals += np.random.normal(0, std_val * 0.5, periods)\n",
    "            elif regressor == 'flow':\n",
    "                # Flujo relativamente estable con peque√±as variaciones\n",
    "                future_vals = np.full(periods, mean_val) + np.random.normal(0, std_val * 0.3, periods)\n",
    "            elif regressor == 'vibration':\n",
    "                # Vibraci√≥n con tendencia ascendente leve (desgaste)\n",
    "                trend = np.linspace(0, 0.5, periods)\n",
    "                future_vals = mean_val + trend + np.random.normal(0, std_val * 0.2, periods)\n",
    "            else:\n",
    "                # Otros regresores: valores estables\n",
    "                future_vals = np.full(periods, mean_val) + np.random.normal(0, std_val * 0.3, periods)\n",
    "            \n",
    "            df_future[regressor] = future_vals\n",
    "    \n",
    "    # Combinar datos hist√≥ricos y futuros para predicci√≥n\n",
    "    df_predict = pd.concat([\n",
    "        df_full[['ds', 'y'] + (regressors or [])],\n",
    "        df_future\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    # Generar predicciones\n",
    "    future_forecast = model.predict(df_predict)\n",
    "    \n",
    "    # Extraer solo las predicciones futuras\n",
    "    future_predictions = future_forecast.iloc[-periods:].copy()\n",
    "    \n",
    "    return future_predictions, df_future\n",
    "\n",
    "# Generar predicciones futuras\n",
    "future_predictions, df_future = generate_future_predictions(\n",
    "    model, df_raw, periods=168, regressors=key_regressors\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Predicciones futuras generadas!\")\n",
    "print(f\"üìÖ Per√≠odo futuro: {future_predictions['ds'].min()} a {future_predictions['ds'].max()}\")\n",
    "\n",
    "# Visualizar predicciones futuras\n",
    "fig_future = go.Figure()\n",
    "\n",
    "# Datos hist√≥ricos (√∫ltimos 30 d√≠as)\n",
    "recent_data = df_raw.tail(720)\n",
    "fig_future.add_trace(\n",
    "    go.Scatter(x=recent_data['ds'], y=recent_data['y'], \n",
    "              name='Datos Hist√≥ricos', line=dict(color='blue', width=2))\n",
    ")\n",
    "\n",
    "# Predicciones futuras\n",
    "fig_future.add_trace(\n",
    "    go.Scatter(x=future_predictions['ds'], y=future_predictions['yhat1'], \n",
    "              name='Predicciones Futuras', line=dict(color='red', width=3))\n",
    ")\n",
    "\n",
    "# Intervalos de confianza futuros (si est√°n disponibles)\n",
    "if 'yhat1_lower' in future_predictions.columns and 'yhat1_upper' in future_predictions.columns:\n",
    "    fig_future.add_trace(\n",
    "        go.Scatter(x=future_predictions['ds'], y=future_predictions['yhat1_upper'], \n",
    "                  fill=None, mode='lines', line_color='rgba(0,0,0,0)', \n",
    "                  showlegend=False)\n",
    "    )\n",
    "    fig_future.add_trace(\n",
    "        go.Scatter(x=future_predictions['ds'], y=future_predictions['yhat1_lower'], \n",
    "                  fill='tonexty', mode='lines', line_color='rgba(0,0,0,0)', \n",
    "                  name='Intervalo de Confianza', fillcolor='rgba(255,0,0,0.2)')\n",
    "    )\n",
    "\n",
    "# L√≠nea divisoria\n",
    "last_historical_date = df_raw['ds'].max()\n",
    "fig_future.add_vline(x=last_historical_date, line_dash=\"dash\", \n",
    "                    line_color=\"green\", line_width=2,\n",
    "                    annotation_text=\"Inicio Predicciones\")\n",
    "\n",
    "fig_future.update_layout(\n",
    "    title=\"üöÄ SENTINEL - Predicciones Futuras (1 Semana)\",\n",
    "    xaxis_title=\"Fecha\",\n",
    "    yaxis_title=\"Presi√≥n (PSI)\",\n",
    "    height=600,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig_future.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 10. DETECCI√ìN DE ANOMAL√çAS Y ALERTAS\n",
    "# =============================================================================\n",
    "\n",
    "def detect_anomalies_and_alerts(df_test, forecast_test, threshold_factor=2.5):\n",
    "    \"\"\"\n",
    "    Detecta anomal√≠as y genera alertas basadas en los residuales del modelo\n",
    "    \n",
    "    Par√°metros:\n",
    "    - df_test: datos de prueba reales\n",
    "    - forecast_test: predicciones de prueba\n",
    "    - threshold_factor: factor para definir umbral de anomal√≠a\n",
    "    \n",
    "    Retorna:\n",
    "    - DataFrame con anomal√≠as detectadas\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üö® Detectando anomal√≠as y generando alertas...\")\n",
    "    \n",
    "    # Calcular residuales\n",
    "    residuals = df_test['y'].values - forecast_test['yhat1'].values\n",
    "    \n",
    "    # Calcular umbrales estad√≠sticos\n",
    "    residual_mean = np.mean(residuals)\n",
    "    residual_std = np.std(residuals)\n",
    "    \n",
    "    upper_threshold = residual_mean + threshold_factor * residual_std\n",
    "    lower_threshold = residual_mean - threshold_factor * residual_std\n",
    "    \n",
    "    # Detectar anomal√≠as\n",
    "    anomalies = []\n",
    "    \n",
    "    for i, (_, row) in enumerate(df_test.iterrows()):\n",
    "        residual = residuals[i]\n",
    "        prediction = forecast_test.iloc[i]['yhat1']\n",
    "        actual = row['y']\n",
    "        \n",
    "        if residual > upper_threshold or residual < lower_threshold:\n",
    "            severity = \"CR√çTICA\" if abs(residual) > threshold_factor * 1.5 * residual_std else \"ALTA\"\n",
    "            \n",
    "            anomaly = {\n",
    "                'timestamp': row['ds'],\n",
    "                'valor_real': actual,\n",
    "                'valor_predicho': prediction,\n",
    "                'residual': residual,\n",
    "                'severidad': severity,\n",
    "                'tipo': 'ALTA' if residual > upper_threshold else 'BAJA',\n",
    "                'descripcion': f\"Presi√≥n {'superior' if residual > 0 else 'inferior'} a lo esperado\"\n",
    "            }\n",
    "            anomalies.append(anomaly)\n",
    "    \n",
    "    df_anomalies = pd.DataFrame(anomalies)\n",
    "    \n",
    "    print(f\"üéØ Resultados de detecci√≥n de anomal√≠as:\")\n",
    "    print(f\"  ‚Ä¢ Total de anomal√≠as detectadas: {len(df_anomalies)}\")\n",
    "    print(f\"  ‚Ä¢ Umbral superior: +{upper_threshold:.2f}\")\n",
    "    print(f\"  ‚Ä¢ Umbral inferior: {lower_threshold:.2f}\")\n",
    "    \n",
    "    if len(df_anomalies) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Distribuci√≥n por severidad:\")\n",
    "        severity_counts = df_anomalies['severidad'].value_counts()\n",
    "        for severity, count in severity_counts.items():\n",
    "            print(f\"    ‚Ä¢ {severity}: {count} eventos\")\n",
    "        \n",
    "        print(f\"\\nüîç Primeras 5 anomal√≠as detectadas:\")\n",
    "        for _, anomaly in df_anomalies.head().iterrows():\n",
    "            print(f\"    ‚Ä¢ {anomaly['timestamp']}: {anomaly['descripcion']} \"\n",
    "                  f\"(Real: {anomaly['valor_real']:.2f}, Pred: {anomaly['valor_predicho']:.2f})\")\n",
    "    \n",
    "    return df_anomalies, upper_threshold, lower_threshold\n",
    "\n",
    "# Detectar anomal√≠as\n",
    "df_anomalies, upper_thresh, lower_thresh = detect_anomalies_and_alerts(\n",
    "    df_test, forecast_test, threshold_factor=2.5\n",
    ")\n",
    "\n",
    "# Visualizar anomal√≠as\n",
    "def plot_anomalies(df_test, forecast_test, df_anomalies, upper_thresh, lower_thresh):\n",
    "    \"\"\"Visualizar anomal√≠as detectadas\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=['üéØ Serie Temporal con Anomal√≠as Detectadas', \n",
    "                       'üìä Residuales y Umbrales de Detecci√≥n'],\n",
    "        vertical_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # Serie temporal con anomal√≠as\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_test['ds'], y=df_test['y'], \n",
    "                  name='Valores Reales', line=dict(color='blue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=forecast_test['ds'], y=forecast_test['yhat1'], \n",
    "                  name='Predicciones', line=dict(color='red', dash='dot')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Marcar anomal√≠as\n",
    "    if len(df_anomalies) > 0:\n",
    "        anomalies_critical = df_anomalies[df_anomalies['severidad'] == 'CR√çTICA']\n",
    "        anomalies_high = df_anomalies[df_anomalies['severidad'] == 'ALTA']\n",
    "        \n",
    "        if len(anomalies_critical) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=anomalies_critical['timestamp'], \n",
    "                          y=anomalies_critical['valor_real'],\n",
    "                          mode='markers', name='Anomal√≠as CR√çTICAS',\n",
    "                          marker=dict(color='red', size=12, symbol='x')),\n",
    "                row=1, col=1\n",
    "            )\n",
    "        \n",
    "        if len(anomalies_high) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=anomalies_high['timestamp'], \n",
    "                          y=anomalies_high['valor_real'],\n",
    "                          mode='markers', name='Anomal√≠as ALTAS',\n",
    "                          marker=dict(color='orange', size=8, symbol='triangle-up')),\n",
    "                row=1, col=1\n",
    "            )\n",
    "    \n",
    "    # Residuales con umbrales\n",
    "    residuals = df_test['y'].values - forecast_test['yhat1'].values\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_test['ds'], y=residuals, \n",
    "                  mode='markers', name='Residuales',\n",
    "                  marker=dict(color='gray', size=4, opacity=0.6)),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Umbrales\n",
    "    fig.add_hline(y=upper_thresh, line_dash=\"dash\", line_color=\"red\", \n",
    "                 annotation_text=\"Umbral Superior\", row=2, col=1)\n",
    "    fig.add_hline(y=lower_thresh, line_dash=\"dash\", line_color=\"red\", \n",
    "                 annotation_text=\"Umbral Inferior\", row=2, col=1)\n",
    "    fig.add_hline(y=0, line_color=\"black\", line_width=1, row=2, col=1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"üö® SENTINEL - Detecci√≥n de Anomal√≠as\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Presi√≥n (PSI)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Residuales\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Fecha\", row=2, col=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "if len(df_anomalies) > 0:\n",
    "    anomalies_fig = plot_anomalies(df_test, forecast_test, df_anomalies, upper_thresh, lower_thresh)\n",
    "    anomalies_fig.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 11. RESUMEN EJECUTIVO Y M√âTRICAS FINALES\n",
    "# =============================================================================\n",
    "\n",
    "def generate_executive_summary(train_metrics, test_metrics, df_anomalies, df_raw):\n",
    "    \"\"\"\n",
    "    Genera un resumen ejecutivo de los resultados del modelo SENTINEL\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ SENTINEL - RESUMEN EJECUTIVO DEL MODELO NEURALPROPHET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìä INFORMACI√ìN DEL DATASET:\")\n",
    "    print(f\"    ‚Ä¢ Total de registros: {len(df_raw):,}\")\n",
    "    print(f\"    ‚Ä¢ Per√≠odo analizado: {df_raw['ds'].min()} a {df_raw['ds'].max()}\")\n",
    "    print(f\"    ‚Ä¢ Frecuencia: Horaria\")\n",
    "    print(f\"    ‚Ä¢ Variables predictoras: Presi√≥n (objetivo), Temperatura, Flujo, Vibraci√≥n\")\n",
    "    \n",
    "    print(f\"\\nüéØ DESEMPE√ëO DEL MODELO:\")\n",
    "    print(f\"    üìà ENTRENAMIENTO:\")\n",
    "    print(f\"        ‚Ä¢ MAE (Error Absoluto Medio): {train_metrics['MAE']:.4f} PSI\")\n",
    "    print(f\"        ‚Ä¢ RMSE (Error Cuadr√°tico Medio): {train_metrics['RMSE']:.4f} PSI\")\n",
    "    print(f\"        ‚Ä¢ MAPE (Error Porcentual): {train_metrics['MAPE']:.2f}%\")\n",
    "    print(f\"        ‚Ä¢ R¬≤ (Coeficiente Determinaci√≥n): {train_metrics['R¬≤']:.4f}\")\n",
    "    \n",
    "    print(f\"    üß™ PRUEBA:\")\n",
    "    print(f\"        ‚Ä¢ MAE (Error Absoluto Medio): {test_metrics['MAE']:.4f} PSI\")\n",
    "    print(f\"        ‚Ä¢ RMSE (Error Cuadr√°tico Medio): {test_metrics['RMSE']:.4f} PSI\")\n",
    "    print(f\"        ‚Ä¢ MAPE (Error Porcentual): {test_metrics['MAPE']:.2f}%\")\n",
    "    print(f\"        ‚Ä¢ R¬≤ (Coeficiente Determinaci√≥n): {test_metrics['R¬≤']:.4f}\")\n",
    "    \n",
    "    # Evaluaci√≥n de calidad\n",
    "    if test_metrics['MAPE'] < 5:\n",
    "        quality = \"EXCELENTE üåü\"\n",
    "    elif test_metrics['MAPE'] < 10:\n",
    "        quality = \"BUENA ‚úÖ\"\n",
    "    elif test_metrics['MAPE'] < 15:\n",
    "        quality = \"ACEPTABLE ‚ö†Ô∏è\"\n",
    "    else:\n",
    "        quality = \"NECESITA MEJORA ‚ùå\"\n",
    "    \n",
    "    print(f\"    üèÜ CALIDAD DEL MODELO: {quality}\")\n",
    "    \n",
    "    print(f\"\\nüö® DETECCI√ìN DE ANOMAL√çAS:\")\n",
    "    if len(df_anomalies) > 0:\n",
    "        print(f\"    ‚Ä¢ Total de anomal√≠as detectadas: {len(df_anomalies)}\")\n",
    "        severity_counts = df_anomalies['severidad'].value_counts()\n",
    "        for severity, count in severity_counts.items():\n",
    "            print(f\"    ‚Ä¢ {severity}: {count} eventos\")\n",
    "        \n",
    "        # Calcular tasa de anomal√≠as\n",
    "        anomaly_rate = (len(df_anomalies) / len(df_test)) * 100\n",
    "        print(f\"    ‚Ä¢ Tasa de anomal√≠as: {anomaly_rate:.2f}%\")\n",
    "    else:\n",
    "        print(f\"    ‚Ä¢ No se detectaron anomal√≠as significativas\")\n",
    "    \n",
    "    print(f\"\\nüîß CONFIGURACI√ìN DEL MODELO:\")\n",
    "    print(f\"    ‚Ä¢ Algoritmo: NeuralProphet\")\n",
    "    print(f\"    ‚Ä¢ Lags autorregresivos: 48 horas\")\n",
    "    print(f\"    ‚Ä¢ Variables ex√≥genas: Temperatura, Flujo, Vibraci√≥n\")\n",
    "    print(f\"    ‚Ä¢ Estacionalidades: Diaria, Semanal, Anual\")\n",
    "    print(f\"    ‚Ä¢ √âpocas de entrenamiento: 100\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ FORTALEZAS DEL MODELO:\")\n",
    "    print(f\"    ‚Ä¢ Captura patrones estacionales complejos\")\n",
    "    print(f\"    ‚Ä¢ Integra m√∫ltiples variables ex√≥genas\")\n",
    "    print(f\"    ‚Ä¢ Genera intervalos de confianza\")\n",
    "    print(f\"    ‚Ä¢ Detecci√≥n autom√°tica de anomal√≠as\")\n",
    "    print(f\"    ‚Ä¢ Escalable para producci√≥n\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  √ÅREAS DE MEJORA:\")\n",
    "    print(f\"    ‚Ä¢ Validaci√≥n con datos reales de sensores\")\n",
    "    print(f\"    ‚Ä¢ Optimizaci√≥n de hiperpar√°metros\")\n",
    "    print(f\"    ‚Ä¢ Incorporaci√≥n de m√°s variables contextuales\")\n",
    "    print(f\"    ‚Ä¢ Evaluaci√≥n en diferentes escenarios operativos\")\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'model_quality': quality,\n",
    "        'test_mape': test_metrics['MAPE'],\n",
    "        'anomalies_detected': len(df_anomalies),\n",
    "        'anomaly_rate': (len(df_anomalies) / len(df_test)) * 100 if len(df_anomalies) > 0 else 0\n",
    "    }\n",
    "\n",
    "# Generar resumen ejecutivo\n",
    "executive_summary = generate_executive_summary(train_metrics, test_metrics, df_anomalies, df_raw)\n",
    "\n",
    "# =============================================================================\n",
    "# 12. EXPORTACI√ìN DE RESULTADOS Y MODELO\n",
    "# =============================================================================\n",
    "\n",
    "def export_results_and_model(model, forecast, df_anomalies, metrics):\n",
    "    \"\"\"\n",
    "    Exporta resultados y guarda el modelo para uso posterior\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüíæ Exportando resultados...\")\n",
    "    \n",
    "    # Crear directorio de resultados\n",
    "    import os\n",
    "    results_dir = \"sentinel_results\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "    \n",
    "    # Exportar predicciones\n",
    "    forecast_export = forecast[['ds', 'yhat1']].copy()\n",
    "    forecast_export.columns = ['timestamp', 'predicted_pressure']\n",
    "    forecast_export.to_csv(f\"{results_dir}/sentinel_predictions.csv\", index=False)\n",
    "    \n",
    "    # Exportar anomal√≠as\n",
    "    if len(df_anomalies) > 0:\n",
    "        df_anomalies.to_csv(f\"{results_dir}/sentinel_anomalies.csv\", index=False)\n",
    "    \n",
    "    # Crear reporte de m√©tricas\n",
    "    metrics_report = {\n",
    "        'model_name': 'NeuralProphet_SENTINEL_Baseline',\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'anomalies_count': len(df_anomalies)\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(f\"{results_dir}/sentinel_metrics_report.json\", 'w') as f:\n",
    "        json.dump(metrics_report, f, indent=2, default=str)\n",
    "    \n",
    "    # Guardar modelo (serialize)\n",
    "    # Nota: NeuralProphet usa PyTorch internamente\n",
    "    try:\n",
    "        import torch\n",
    "        torch.save(model.model.state_dict(), f\"{results_dir}/sentinel_model_weights.pth\")\n",
    "        \n",
    "        # Guardar configuraci√≥n del modelo\n",
    "        model_config = {\n",
    "            'n_lags': 48,\n",
    "            'regressors': key_regressors,\n",
    "            'seasonalities': ['yearly', 'weekly', 'daily'],\n",
    "            'training_params': {\n",
    "                'epochs': 100,\n",
    "                'batch_size': 64,\n",
    "                'learning_rate': 0.01\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(f\"{results_dir}/sentinel_model_config.json\", 'w') as f:\n",
    "            json.dump(model_config, f, indent=2)\n",
    "            \n",
    "        print(f\"‚úÖ Modelo guardado en: {results_dir}/\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error al guardar modelo: {e}\")\n",
    "    \n",
    "    print(f\"üìÅ Archivos exportados en directorio: {results_dir}/\")\n",
    "    print(f\"    ‚Ä¢ sentinel_predictions.csv\")\n",
    "    print(f\"    ‚Ä¢ sentinel_anomalies.csv\")\n",
    "    print(f\"    ‚Ä¢ sentinel_metrics_report.json\")\n",
    "    print(f\"    ‚Ä¢ sentinel_model_weights.pth\")\n",
    "    print(f\"    ‚Ä¢ sentinel_model_config.json\")\n",
    "    \n",
    "    return results_dir\n",
    "\n",
    "# Exportar resultados\n",
    "results_directory = export_results_and_model(model, forecast, df_anomalies, test_metrics)\n",
    "\n",
    "# =============================================================================\n",
    "# 13. FUNCI√ìN PARA PREDICCI√ìN EN TIEMPO REAL\n",
    "# =============================================================================\n",
    "\n",
    "def create_realtime_prediction_function(model, regressors):\n",
    "    \"\"\"\n",
    "    Crea una funci√≥n optimizada para predicci√≥n en tiempo real\n",
    "    que puede ser integrada con la API de SENTINEL\n",
    "    \"\"\"\n",
    "    \n",
    "    def predict_next_values(recent_data, hours_ahead=24):\n",
    "        \"\"\"\n",
    "        Funci√≥n para predicci√≥n en tiempo real\n",
    "        \n",
    "        Par√°metros:\n",
    "        - recent_data: DataFrame con datos recientes (m√≠nimo 48 horas)\n",
    "        - hours_ahead: horas a predecir hacia adelante\n",
    "        \n",
    "        Retorna:\n",
    "        - DataFrame con predicciones\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Validar datos de entrada\n",
    "            required_cols = ['ds', 'y'] + regressors\n",
    "            if not all(col in recent_data.columns for col in required_cols):\n",
    "                raise ValueError(f\"Faltan columnas requeridas: {required_cols}\")\n",
    "            \n",
    "            if len(recent_data) < 48:\n",
    "                raise ValueError(\"Se requieren al menos 48 horas de datos hist√≥ricos\")\n",
    "            \n",
    "            # Preparar datos para predicci√≥n\n",
    "            df_predict = recent_data[required_cols].copy()\n",
    "            df_predict = df_predict.sort_values('ds').reset_index(drop=True)\n",
    "            \n",
    "            # Generar predicciones\n",
    "            predictions = model.predict(df_predict)\n",
    "            \n",
    "            # Extraer √∫ltimas predicciones (futuras)\n",
    "            future_predictions = predictions.tail(hours_ahead)\n",
    "            \n",
    "            # Formatear salida\n",
    "            result = future_predictions[['ds', 'yhat1']].copy()\n",
    "            result.columns = ['timestamp', 'predicted_pressure']\n",
    "            result['confidence_lower'] = future_predictions.get('yhat1_lower', None)\n",
    "            result['confidence_upper'] = future_predictions.get('yhat1_upper', None)\n",
    "            \n",
    "            return result.to_dict('records')\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    return predict_next_values\n",
    "\n",
    "# Crear funci√≥n de predicci√≥n en tiempo real\n",
    "realtime_predictor = create_realtime_prediction_function(model, key_regressors)\n",
    "\n",
    "print(\"\\nüîß Funci√≥n de predicci√≥n en tiempo real creada!\")\n",
    "print(\"üí° Ejemplo de uso:\")\n",
    "print(\"\"\"\n",
    "# Datos recientes de sensores\n",
    "recent_sensor_data = df_raw.tail(72)  # √öltimas 72 horas\n",
    "\n",
    "# Generar predicci√≥n para pr√≥ximas 24 horas\n",
    "predictions = realtime_predictor(recent_sensor_data, hours_ahead=24)\n",
    "\n",
    "# Resultado: lista de diccionarios con timestamp y predicted_pressure\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINALIZACI√ìN DEL NOTEBOOK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"üéâ\" * 20)\n",
    "print(\"‚úÖ NOTEBOOK SENTINEL - NEURALPROPHET COMPLETADO EXITOSAMENTE!\")\n",
    "print(\"üéâ\" * 20)\n",
    "\n",
    "print(f\"\\nüìã RESUMEN FINAL:\")\n",
    "print(f\"    ‚Ä¢ Modelo baseline establecido con NeuralProphet\")\n",
    "print(f\"    ‚Ä¢ MAPE en prueba: {test_metrics['MAPE']:.2f}%\")\n",
    "print(f\"    ‚Ä¢ Anomal√≠as detectadas: {len(df_anomalies)}\")\n",
    "print(f\"    ‚Ä¢ Funci√≥n de predicci√≥n en tiempo real creada\")\n",
    "print(f\"    ‚Ä¢ Resultados exportados en: {results_directory}/\")\n",
    "\n",
    "print(f\"\\nüöÄ SIGUIENTE FASE:\")\n",
    "print(f\"    ‚Ä¢ Integrar con API REST de SENTINEL\")\n",
    "print(f\"    ‚Ä¢ Desarrollar dashboard de visualizaci√≥n\")\n",
    "print(f\"    ‚Ä¢ Implementar sistema de alertas autom√°ticas\")\n",
    "print(f\"    ‚Ä¢ Evaluar modelo TFT personalizado para comparaci√≥n\")\n",
    "\n",
    "print(f\"\\nüìß Para dudas o mejoras, contactar al equipo de desarrollo de SENTINEL\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
